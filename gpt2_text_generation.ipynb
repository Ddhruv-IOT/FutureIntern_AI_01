{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFZz7AkfJPaBTXMJa2CZJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ddhruv-IOT/FutureIntern_AI_01/blob/main/gpt2_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb0_sz0MxgJp",
        "outputId": "9bcda48b-ab1d-47c3-9d3f-ae762c4649e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "7DKzrqu0xmOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "pMCvXj6UxvTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/training_dataset.txt',  # Replace with your dataset path\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdhXMdEox2Ex",
        "outputId": "566f7062-5e31-48cf-88c2-d5171d489234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=300,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "fqZwyQRmyCYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('./fine-tuned-gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "u7YmJ7wmyKO7",
        "outputId": "ddc61815-92e7-4c4f-c5af-3a9a1aee0b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 1:17:55, Epoch 300/300]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.089500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"What is Arduino ?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Greedy Search\n",
        "greedy_output = model.generate(input_ids, max_length=100)\n",
        "print(\"Greedy Search Output:\\n\" + tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMIRnh86yN8d",
        "outputId": "7f93c55d-76d6-4005-92de-e38be8f841bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Search Output:\n",
            "What is Arduino?\n",
            "\n",
            "Arduino is an open-source electronics platform based on easy-to-use hardware and software. It's designed to make electronics more accessible to artists, designers, hobbyists, and anyone interested in creating interactive projects. The core of the Arduino platform is the Arduino board, a small microcontroller that can be programmed to sense and control objects in the physical world.\n",
            "\n",
            "The Arduino platform has gained immense popularity due to its simplicity and versatility. It allows users to create\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is Arduino ?\"\n",
        "\n",
        "# Encode the input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate text with all techniques combined\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,            # Maximum length of the generated text\n",
        "    num_beams=5,               # Beam search width\n",
        "    temperature=0.7,           # Temperature for controlling randomness\n",
        "    top_k=50,                  # Top-k sampling\n",
        "    top_p=0.9,                 # Top-p (nucleus) sampling\n",
        "    no_repeat_ngram_size=2,    # n-gram penalty to prevent repetition\n",
        "    early_stopping=True        # Stop when a beam hits an end token\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZOlm3Ha0YS4",
        "outputId": "7b70a978-b140-46ab-ff28-97f42af59efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Arduino?\n",
            "\n",
            "Arduino is an open-source electronics platform based on easy-to-use hardware and software. It's designed to make electronics more accessible to artists, designers, hobbyists, and anyone interested in creating interactive projects. The beauty of Arduino lies in its ability to bridge the gap between software and hardware, enabling even those without a computer to create projects and projects that are easy to learn and use.\n",
            "In conclusion, Arduino is a versatile and accessible platform that has revolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is Arduino ?\"\n",
        "\n",
        "# Encode the input prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate text with all techniques combined\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=100,            # Maximum length of the generated text\n",
        "    num_beams=2,               # Beam search width\n",
        "    temperature=0.9,           # Temperature for controlling randomness\n",
        "    top_k=50,                  # Top-k sampling\n",
        "    top_p=0.9,                 # Top-p (nucleus) sampling\n",
        "    no_repeat_ngram_size=2,    # n-gram penalty to prevent repetition\n",
        "    early_stopping=True        # Stop when a beam hits an end token\n",
        ")\n",
        "\n",
        "# Decode and print the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MikR7XrHvnkD",
        "outputId": "08170004-1149-4ffa-f370-b59831cde2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Arduino?\n",
            "\n",
            "Arduino is an open-source electronics platform based on easy-to-use hardware and software. It's designed to make electronics more accessible to artists, designers, hobbyists, and anyone interested in creating interactive projects. The core of Arduino is the Arduino board, a small microcontroller that can be programmed to sense and control objects in the physical world.\n",
            "- OS: Windows, Linux, FreeBSD, or other Unix-compatible platforms (including Solaris) are supported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfoyt0XuwGs1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}